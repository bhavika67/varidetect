

https://github.com/user-attachments/assets/e1827fa1-9722-4c0b-b755-f03c87e585ab

# VariDetect â€“ Multi-Modal Deepfake Detection Android App

VariDetect is a **multi-modal deepfake detection Android application** capable of analyzing **images**, **videos**, and **audio** to determine whether the content is real or AI-manipulated.  
It integrates **three separate deep learning models**, each trained for a different media type, and deploys them on-device using **TensorFlow Lite** for fast, offline inference.

This project demonstrates:
- Multi-modal ML systems (image + video + audio)
- CNN & CNN+LSTM architectures
- Audio deepfake detection via spectrograms
- TFLite conversion and on-device inference
- Android development with Java/Kotlin
- End-to-end ML deployment on mobile

---

# ğŸ¯ Features

- **Image Deepfake Detection** using CNN model  
- **Video Deepfake Detection** using CNN + LSTM  
- **Audio Deepfake Detection** using spectrogram-based CNN  
- **On-Device ML Inference** (no server required)  
- **Real/Fake Classification + Confidence Score**  
- **Offline & Privacy-Friendly**  
- **User-Friendly Android UI**  

---

# ğŸ§  ML Model Architecture

### ğŸ”¹ **Image Model â€“ CNN**
- Preprocessing: resize â†’ normalize  
- Architecture: EfficientNetB4 / Custom CNN  
- Output: softmax probability (real vs fake)

---

### ğŸ”¹ **Video Model â€“ CNN + LSTM**
- Extract frames from the video  
- CNN â†’ spatial feature extraction  
- LSTM â†’ temporal sequence modeling  
- Aggregate predictions across frames

---

### ğŸ”¹ **Audio Model â€“ Spectrogram CNN**
- Convert audio â†’ Mel Spectrogram  
- CNN classifies real vs synthetic voices  
- Supports WAV/MP3 input formats

---

# ğŸ”— Model Training Repository
All training notebooks, experiments, and TFLite conversion scripts are available here:

ğŸ‘‰ **Model Repository:**  
https://github.com/bhavika67/deepfake_ml_model

This includes:
- Image model training  
- Video frame extraction + CNN + LSTM  
- Audio spectrogram model  
- Evaluation metrics  
- TFLite conversion  
- Dataset usage  

---

# ğŸš€ TensorFlow Lite Deployment

Each model was exported to TFLite using:

```bash
tflite_convert \
 --saved_model_dir=model/ \
 --output_file=model.tflite \
 --optimize=DEFAULT
```

### Android inference uses:
- **TFLite Interpreter**  
- ByteBuffer input conversion  
- Float32 tensors  
- Softmax output decoding  

All inference runs **offline**, making the app faster & more privacy-friendly.

---

# ğŸ“ Project Structure

```
varidetect/
â”‚
â”œâ”€â”€ android_app/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ src/main/
â”‚   â”‚   â”‚   â”œâ”€â”€ assets/              # TFLite models (.tflite)
â”‚   â”‚   â”‚   â”œâ”€â”€ java/com/vari/varidetect/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Activity/        # Splash, Home, OnBoard, etc.
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Fragment/        # Upload, Result, FAQ, History, etc.
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Adapter/         # RecyclerView adapters
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ HelperClass/     # ML inference helpers
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Model/           # Data models
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Utils/           # Utilities & managers
â”‚   â”‚   â”‚   â”œâ”€â”€ res/                 # Layouts, icons, drawables, fonts
â”‚   â”‚   â”‚   â””â”€â”€ AndroidManifest.xml
â”‚   â”‚   â””â”€â”€ test/
â”‚   â””â”€â”€ build.gradle
â”‚
â”œâ”€â”€ ml-pipeline/                      # Documentation for the ML models
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ samples/
â”‚   â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ videos/
â”‚   â””â”€â”€ audio/
â”‚
â””â”€â”€ README.md
```

---

# ğŸ”§ How Detection Works

### ğŸ–¼ **Image Detection**
1. Upload/select image  
2. Preprocess (resize â†’ normalize)  
3. Run through image TFLite model  
4. Get probability output  

---

### ğŸ¥ **Video Detection**
1. Extract frames from selected video  
2. Preprocess each frame  
3. CNN generates spatial features  
4. LSTM processes frame sequence  
5. Aggregate predictions  

---

### ğŸ¤ **Audio Detection**
1. Convert audio â†’ Mel Spectrogram  
2. Resize to CNN input dimensions  
3. Run spectrogram through TFLite model  
4. Output = real/fake probability  

---

# ğŸ›  How to Run the App

### Android Setup
1. Clone this repository  
2. Open `/android_app` in **Android Studio**  
3. Add all `.tflite` models to:

```
android_app/app/src/main/assets/
```

4. Build & run on emulator/device  
5. Choose Image / Video / Audio  
6. View detection result  

---

# ğŸ“Š Example Outputs

| Media Type | Output | Confidence |
|------------|--------|------------|
| Image      | Fake   | 0.91       |
| Video      | Real   | 0.73       |
| Audio      | Fake   | 0.88       |

---

# ğŸ“š Datasets Used
- **FaceForensics++** (image/video)  
- **WaveFake / LJ Speech** (audio)  
- Additional synthetic augmentations  

Only **public datasets** were used.

---

# âš  Limitations
- Video inference may be slow on low-end devices  
- Audio quality impacts accuracy  
- No multi-modal fusion (models run independently)  
- Larger TFLite models increase app size  

---

# ğŸ§  What I Learned
- Designing multi-modal ML systems  
- Building CNN, LSTM & spectrogram models  
- TFLite conversion & optimization  
- Preprocessing pipelines for image/video/audio  
- Android ML integration  
- Mobile performance constraints  

---

# ğŸ¤ Contributing
PRs and suggestions are welcome.

---

# ğŸ“œ License
MIT License
SE)**.   
